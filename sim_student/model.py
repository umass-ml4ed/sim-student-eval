from typing import Union, Tuple, List, Optional
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, PreTrainedModel, PreTrainedTokenizer
from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training
from vllm import LLM, SamplingParams, EngineArgs, LLMEngine
from vllm.lora.request import LoRARequest
from openai_harmony import (
    HarmonyEncodingName,
    load_harmony_encoding,
    Conversation,
    Message,
    Role,
    SystemContent,
    DeveloperContent,
)

from sim_student.utils import get_checkpoint_path

BASE_MODEL_NAME_MAP = {
    "1b": "meta-llama/Llama-3.2-1B-Instruct",
    "3b": "meta-llama/Llama-3.2-3B-Instruct",
    "8b": "meta-llama/Llama-3.1-8B-Instruct",
    "70b": "meta-llama/Llama-3.1-70B-Instruct"
}

def get_tokenizer(base_model_name: str) -> PreTrainedTokenizer:
    base_model_name = BASE_MODEL_NAME_MAP.get(base_model_name, base_model_name)
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    tokenizer.pad_token = "<|finetune_right_pad_id|>" # Special padding token for Llama
    return tokenizer

def get_base_model(base_model_name: str, quantize: bool) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:
    base_model_name = BASE_MODEL_NAME_MAP.get(base_model_name, base_model_name)
    tokenizer = get_tokenizer(base_model_name)
    print(f"Loading model {'with' if quantize else 'without'} quanitization: {base_model_name}")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        pad_token_id=tokenizer.pad_token_id,
        quantization_config=BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True
        ) if quantize else None,
        # f32 seems helpful for train/test time consistency when quantizing, bf16 performs best for non-quantized
        dtype=torch.float32 if quantize else torch.bfloat16,
        device_map={"": 0}
    )
    base_model.config.use_cache = False
    base_model.config.pretraining_tp = 1
    return base_model, tokenizer

def get_model(model: PreTrainedModel, test: bool,
              model_name: Union[str, List[Tuple[str, str]]] = None, pt_model_name: str = None,
              r: int = None, lora_alpha: int = None,
              quantize: bool = False, use_gradient_checkpointing: bool = True) -> Union[PeftModel, PreTrainedModel]:
    if test and model_name:
        # Note we are loading adapter on quantized model and not merging
        # Recommended here - https://huggingface.co/docs/trl/main/en/dpo_trainer#downsides-to-merging-qlora-before-dpo-approach-2
        # Also prevents empty responses generated by Llama models
        if isinstance(model_name, str):
            model_name = [(model_name, "default")]
        for adpt_idx, (mn, an) in enumerate(model_name):
            print(f"Initializing inference-time model from fine-tuned LoRA adapter ({an}): {mn}")
            if adpt_idx == 0:
                model = PeftModel.from_pretrained(model, get_checkpoint_path(mn), adapter_name=an)
            else:
                model.load_adapter(get_checkpoint_path(mn), adapter_name=an)
    elif not test:
        # NOTE: this seems to slow things down and doesn't seem to be necessary
        if quantize:
            model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=use_gradient_checkpointing)
        if pt_model_name:
            print(f"Initializing trainable model from pre-trained LoRA adapters: {pt_model_name}")
            model = PeftModel.from_pretrained(model, get_checkpoint_path(pt_model_name), is_trainable=True, adapter_name="default")
            model.load_adapter(get_checkpoint_path(pt_model_name), is_trainable=False, adapter_name="lora_ref")
        else:
            print("Initializing trainable model with new LoRA adapters")
            peft_config = LoraConfig(
                target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
                r=r,
                lora_alpha=lora_alpha,
                lora_dropout=0.05,
                task_type="CAUSAL_LM",
                inference_mode=False,
                use_rslora=True
            )
            model = get_peft_model(model, peft_config)
    else:
        print("Using inference-time model with pre-trained weights")
    return model

def generate_vllm(base_model: Union[str, LLM], prompts: Union[List[str], List[List[int]]],
                  adapter: Union[Tuple[str, int, str], str, None], sampling_params: dict) -> List[str]:
    if isinstance(base_model, LLM):
        model_name = base_model.llm_engine.get_model_config().model
    else:
        base_model = BASE_MODEL_NAME_MAP.get(base_model, base_model)
        model_name = base_model
    gpt_oss = model_name.startswith("openai/gpt-oss")
    if gpt_oss:
        # Reference: https://cookbook.openai.com/articles/gpt-oss/run-vllm
        # Harmony stop tokens (pass to sampler so they won't be included in output)
        encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)
        stop_token_ids = encoding.stop_tokens_for_assistant_actions()
        sampling_params = SamplingParams(**sampling_params, stop_token_ids=stop_token_ids)
    else:
        sampling_params = SamplingParams(**sampling_params)

    if adapter:
        # Reference: https://docs.vllm.ai/en/v0.8.0/getting_started/examples/multilora_inference.html
        print("Using LoRA adapter for generation:", adapter)
        if isinstance(adapter, str):
            adapter_id, adapter_idx, adapter_name = ("default", 1, adapter)
        else:
            adapter_id, adapter_idx, adapter_name = adapter
        if isinstance(base_model, str):
            model = LLM(base_model, enable_lora=True, max_lora_rank=256)
        else:
            model = base_model
        adapter_path = get_checkpoint_path(adapter_name)
        outputs = model.generate(prompts, sampling_params=sampling_params, lora_request=LoRARequest(adapter_id, adapter_idx, adapter_path))
    else:
        print("Using base model for generation:", model_name)
        if isinstance(base_model, str):
            model = LLM(base_model)
        else:
            model = base_model
        if gpt_oss:
            outputs = model.generate(prompt_token_ids=prompts, sampling_params=sampling_params)
        else:
            outputs = model.generate(prompts, sampling_params=sampling_params)
    return [completion.text for output in outputs for completion in output.outputs]

def tokenize_prompts(model_name: str, prompts: List[str], system_prompt: str):
    if model_name.startswith("openai/gpt-oss"):
        return tokenize_gpt_oss_prompts(prompts, system_prompt)
    return tokenize_qwen_prompts(model_name, prompts, system_prompt)

def tokenize_qwen_prompts(model_name: str, prompts: List[str], system_prompt: str):
    tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(model_name)
    prompts_with_template: List[str] = [
        tokenizer.apply_chat_template(
            [{"role": "user", "content": system_prompt + "\n\n" + prompt}],
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=True
        )
        for prompt in prompts
    ]
    return prompts_with_template

def tokenize_gpt_oss_prompts(prompts: List[str], system_prompt: str):
    # Reference: https://cookbook.openai.com/articles/gpt-oss/run-vllm
    encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)
    prefill_ids = [
        encoding.render_conversation_for_completion(
            Conversation.from_messages(
                [
                    Message.from_role_and_content(Role.SYSTEM, SystemContent.new()),
                    Message.from_role_and_content(
                        Role.DEVELOPER,
                        DeveloperContent.new().with_instructions(system_prompt),
                    ),
                    Message.from_role_and_content(Role.USER, prompt),
                ]
            ), Role.ASSISTANT)
        for prompt in prompts
    ]
    return prefill_ids
